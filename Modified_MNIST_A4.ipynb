{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Copy of Copy of Miniproject3_Final_new (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PseRKoH0TQ_R"
      },
      "source": [
        "# **MAIS 202**\n",
        "Team members: Ahmad Ghawanmeh, Feng Xia, Tahseen Bin Taj\n",
        "\n",
        "Team name: Deeply Convoluted Team\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLqHEsJmNI_q"
      },
      "source": [
        "## Loading Required Sources\n",
        "Loading required libraries, tensorboard and google drive link from google colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a55ZRYnxTQ_T"
      },
      "source": [
        "#Importing the required libraries \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "import h5py\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import load_model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, MaxPool2D, Convolution2D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from skimage import img_as_float\n",
        "from skimage.morphology import reconstruction"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucgFRbGSTYvO",
        "outputId": "f1ad7cf6-5d93-4021-ff7f-bac36803843a"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/My Drive/\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv1Rf3coNgK5"
      },
      "source": [
        "# Loading Modified MNIST Dataset and Data Augmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T422QK-f-Hkq"
      },
      "source": [
        "Uncomment these code to download train and test datasets in your google drive if they haven't be downloaded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwYA2ler9w5F"
      },
      "source": [
        "#os.chdir(\"/content/drive/My Drive/\")\n",
        "#from google.colab import files\n",
        "#files.upload() #this will prompt you to update the json\n",
        "\n",
        "#!pip install -q kaggle\n",
        "#!mkdir -p ~/.kaggle\n",
        "#!cp kaggle.json ~/.kaggle/\n",
        "#!ls ~/.kaggle\n",
        "#!chmod 600 /root/.kaggle/kaggle.json  # set permission\n",
        "#!cd ~/.kaggle/kaggle.json\n",
        "\n",
        "#!kaggle competitions download -c mais-202-fall-2020-kaggle-competition\n",
        "\n",
        "#os.chdir('/content/drive/My Drive')  #change dir\n",
        "#!mkdir train  #create a directory named train/\n",
        "#!mkdir test  #create a directory named test/\n",
        "\n",
        "#!unzip -q train_x.npy.zip -d train/  #unzip data in train/\n",
        "#!unzip -q test_x.npy.zip -d test/  #unzip data in test/"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ8jH7jbTQ_W"
      },
      "source": [
        "#Loading the data\n",
        "train_x = np.load(\"./train/train_x.npy\")\n",
        "test_x = np.load(\"./test/test_x.npy\")\n",
        "y_train = pd.read_csv(\"train_y.csv\")['Label']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWIVH6Vm-mnt"
      },
      "source": [
        "# removing the background noise for each image\n",
        "#for i in range(train_x.shape[0]):\n",
        "#  image = img_as_float(train_x[i])\n",
        "#  image = gaussian_filter(image, 0.5)\n",
        "\n",
        "#  seed = np.copy(image)\n",
        "#  seed[1:-1, 1:-1] = image.min()\n",
        "#  mask = image\n",
        "\n",
        "#  dilated = reconstruction(seed, mask, method='dilation')\n",
        "#  dilated_image = image - dilated\n",
        "#  train_x[i] = dilated_image"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMmtvXCzTQ_Y"
      },
      "source": [
        "# One hot encoding\n",
        "y_train = to_categorical(y_train)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25nbgH0UTQ_c"
      },
      "source": [
        "# Reshaping the data\n",
        "train_x=train_x.reshape(train_x.shape[0],128,128,1)\n",
        "test_x=test_x.reshape(test_x.shape[0],128,128,1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJkfOQ0hTQ_e"
      },
      "source": [
        "# splitting the data for training and validating\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_x, y_train, test_size=0.1, random_state=77)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCJeBI9Q6zTZ"
      },
      "source": [
        "# augmenting the pictures\n",
        "aug = ImageDataGenerator(\n",
        "    rotation_range=25, \n",
        "    width_shift_range=0.15,\n",
        "    shear_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.2\n",
        "    )\n",
        "\n",
        "aug.fit(X_train)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RELvrH8fOIi5"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfRk9OToTQ_q"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Convolution2D(32, kernel_size=3, padding=\"same\", activation='relu', input_shape=(128,128,1),\n",
        "                            data_format=\"channels_last\"))\n",
        "model.add(Convolution2D(32, kernel_size=3, padding=\"same\", activation='relu'))\n",
        "model.add(Convolution2D(32, kernel_size=3, padding=\"same\", activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Convolution2D(64, kernel_size=3, padding=\"same\", activation='relu'))\n",
        "model.add(Convolution2D(64, kernel_size=3, padding=\"same\", activation='relu'))\n",
        "model.add(Convolution2D(64, kernel_size=3, padding=\"same\", activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Convolution2D(128, kernel_size=3, padding=\"same\", activation='relu'))\n",
        "model.add(Convolution2D(128, kernel_size=3, padding=\"same\", activation='relu'))\n",
        "model.add(Convolution2D(128, kernel_size=3, padding=\"same\", activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Convolution2D(256, kernel_size=3, padding=\"same\", activation='relu'))\n",
        "model.add(Convolution2D(256, kernel_size=3, padding=\"same\", activation='relu'))\n",
        "model.add(Convolution2D(256, kernel_size=3, padding=\"same\", activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "best_model=\"best_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(best_model, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Eea7PZGMJqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4667c539-e8d1-49b3-89ff-3f0da3d81812"
      },
      "source": [
        "model.fit_generator(aug.flow(X_train, y_train, batch_size=400),\n",
        "                              validation_data=(X_test, y_test),\n",
        "                              epochs = 40, callbacks=callbacks_list )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-68a5f472a939>:3: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/40\n",
            " 2/90 [..............................] - ETA: 13s - loss: 13.2311 - accuracy: 0.1988WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0983s vs `on_train_batch_end` time: 0.2060s). Check your callbacks.\n",
            "90/90 [==============================] - ETA: 0s - loss: 2.9431 - accuracy: 0.2296WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_test_batch_end` time: 0.0060s). Check your callbacks.\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.22375, saving model to best_model.hdf5\n",
            "90/90 [==============================] - 53s 589ms/step - loss: 2.9431 - accuracy: 0.2296 - val_loss: 2.0831 - val_accuracy: 0.2237\n",
            "Epoch 2/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.9566 - accuracy: 0.2606\n",
            "Epoch 00002: val_accuracy improved from 0.22375 to 0.23375, saving model to best_model.hdf5\n",
            "90/90 [==============================] - 53s 593ms/step - loss: 1.9566 - accuracy: 0.2606 - val_loss: 1.9948 - val_accuracy: 0.2338\n",
            "Epoch 3/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.8815 - accuracy: 0.2811\n",
            "Epoch 00003: val_accuracy did not improve from 0.23375\n",
            "90/90 [==============================] - 53s 588ms/step - loss: 1.8815 - accuracy: 0.2811 - val_loss: 1.9989 - val_accuracy: 0.1965\n",
            "Epoch 4/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.7979 - accuracy: 0.3199\n",
            "Epoch 00004: val_accuracy did not improve from 0.23375\n",
            "90/90 [==============================] - 53s 585ms/step - loss: 1.7979 - accuracy: 0.3199 - val_loss: 2.0754 - val_accuracy: 0.1998\n",
            "Epoch 5/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.6994 - accuracy: 0.3559\n",
            "Epoch 00005: val_accuracy improved from 0.23375 to 0.25475, saving model to best_model.hdf5\n",
            "90/90 [==============================] - 53s 591ms/step - loss: 1.6994 - accuracy: 0.3559 - val_loss: 1.9500 - val_accuracy: 0.2548\n",
            "Epoch 6/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.5438 - accuracy: 0.4491\n",
            "Epoch 00006: val_accuracy did not improve from 0.25475\n",
            "90/90 [==============================] - 53s 584ms/step - loss: 1.5438 - accuracy: 0.4491 - val_loss: 2.3262 - val_accuracy: 0.1928\n",
            "Epoch 7/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.2976 - accuracy: 0.5653\n",
            "Epoch 00007: val_accuracy improved from 0.25475 to 0.40625, saving model to best_model.hdf5\n",
            "90/90 [==============================] - 53s 589ms/step - loss: 1.2976 - accuracy: 0.5653 - val_loss: 1.7359 - val_accuracy: 0.4062\n",
            "Epoch 8/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.1092 - accuracy: 0.6269\n",
            "Epoch 00008: val_accuracy improved from 0.40625 to 0.64875, saving model to best_model.hdf5\n",
            "90/90 [==============================] - 54s 595ms/step - loss: 1.1092 - accuracy: 0.6269 - val_loss: 1.0534 - val_accuracy: 0.6488\n",
            "Epoch 9/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.9151 - accuracy: 0.6984\n",
            "Epoch 00009: val_accuracy improved from 0.64875 to 0.70500, saving model to best_model.hdf5\n",
            "90/90 [==============================] - 54s 596ms/step - loss: 0.9151 - accuracy: 0.6984 - val_loss: 0.8458 - val_accuracy: 0.7050\n",
            "Epoch 10/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.8035 - accuracy: 0.7416\n",
            "Epoch 00010: val_accuracy improved from 0.70500 to 0.73725, saving model to best_model.hdf5\n",
            "90/90 [==============================] - 54s 597ms/step - loss: 0.8035 - accuracy: 0.7416 - val_loss: 0.8494 - val_accuracy: 0.7372\n",
            "Epoch 11/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.6968 - accuracy: 0.7824\n",
            "Epoch 00011: val_accuracy improved from 0.73725 to 0.82550, saving model to best_model.hdf5\n",
            "90/90 [==============================] - 53s 594ms/step - loss: 0.6968 - accuracy: 0.7824 - val_loss: 0.5055 - val_accuracy: 0.8255\n",
            "Epoch 12/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.5981 - accuracy: 0.8175\n",
            "Epoch 00012: val_accuracy improved from 0.82550 to 0.87175, saving model to best_model.hdf5\n",
            "90/90 [==============================] - 53s 592ms/step - loss: 0.5981 - accuracy: 0.8175 - val_loss: 0.4387 - val_accuracy: 0.8717\n",
            "Epoch 13/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.5324 - accuracy: 0.8378\n",
            "Epoch 00013: val_accuracy improved from 0.87175 to 0.91500, saving model to best_model.hdf5\n",
            "90/90 [==============================] - 54s 595ms/step - loss: 0.5324 - accuracy: 0.8378 - val_loss: 0.2904 - val_accuracy: 0.9150\n",
            "Epoch 14/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.4990 - accuracy: 0.8492\n",
            "Epoch 00014: val_accuracy improved from 0.91500 to 0.93175, saving model to best_model.hdf5\n",
            "90/90 [==============================] - 53s 593ms/step - loss: 0.4990 - accuracy: 0.8492 - val_loss: 0.2335 - val_accuracy: 0.9317\n",
            "Epoch 15/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.4856 - accuracy: 0.8576\n",
            "Epoch 00015: val_accuracy did not improve from 0.93175\n",
            "90/90 [==============================] - 53s 587ms/step - loss: 0.4856 - accuracy: 0.8576 - val_loss: 0.3847 - val_accuracy: 0.9095\n",
            "Epoch 16/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.4613 - accuracy: 0.8596\n",
            "Epoch 00016: val_accuracy did not improve from 0.93175\n",
            "90/90 [==============================] - 52s 583ms/step - loss: 0.4613 - accuracy: 0.8596 - val_loss: 0.3204 - val_accuracy: 0.9120\n",
            "Epoch 17/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.4197 - accuracy: 0.8741\n",
            "Epoch 00017: val_accuracy improved from 0.93175 to 0.93825, saving model to best_model.hdf5\n",
            "90/90 [==============================] - 53s 586ms/step - loss: 0.4197 - accuracy: 0.8741 - val_loss: 0.2378 - val_accuracy: 0.9383\n",
            "Epoch 18/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.4096 - accuracy: 0.8766\n",
            "Epoch 00018: val_accuracy improved from 0.93825 to 0.94700, saving model to best_model.hdf5\n",
            "90/90 [==============================] - 53s 589ms/step - loss: 0.4096 - accuracy: 0.8766 - val_loss: 0.1941 - val_accuracy: 0.9470\n",
            "Epoch 19/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.4261 - accuracy: 0.8724\n",
            "Epoch 00019: val_accuracy did not improve from 0.94700\n",
            "90/90 [==============================] - 53s 584ms/step - loss: 0.4261 - accuracy: 0.8724 - val_loss: 0.2098 - val_accuracy: 0.9448\n",
            "Epoch 20/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.3757 - accuracy: 0.8887\n",
            "Epoch 00020: val_accuracy improved from 0.94700 to 0.95775, saving model to best_model.hdf5\n",
            "90/90 [==============================] - 53s 592ms/step - loss: 0.3757 - accuracy: 0.8887 - val_loss: 0.1490 - val_accuracy: 0.9578\n",
            "Epoch 21/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.3486 - accuracy: 0.8953\n",
            "Epoch 00021: val_accuracy did not improve from 0.95775\n",
            "90/90 [==============================] - 53s 586ms/step - loss: 0.3486 - accuracy: 0.8953 - val_loss: 0.1527 - val_accuracy: 0.9548\n",
            "Epoch 22/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.3378 - accuracy: 0.8988\n",
            "Epoch 00022: val_accuracy improved from 0.95775 to 0.96900, saving model to best_model.hdf5\n",
            "90/90 [==============================] - 53s 590ms/step - loss: 0.3378 - accuracy: 0.8988 - val_loss: 0.1173 - val_accuracy: 0.9690\n",
            "Epoch 23/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.3244 - accuracy: 0.9030\n",
            "Epoch 00023: val_accuracy did not improve from 0.96900\n",
            "90/90 [==============================] - 52s 581ms/step - loss: 0.3244 - accuracy: 0.9030 - val_loss: 0.1343 - val_accuracy: 0.9620\n",
            "Epoch 24/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.3223 - accuracy: 0.9024\n",
            "Epoch 00024: val_accuracy did not improve from 0.96900\n",
            "90/90 [==============================] - 52s 577ms/step - loss: 0.3223 - accuracy: 0.9024 - val_loss: 0.1440 - val_accuracy: 0.9610\n",
            "Epoch 25/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.3082 - accuracy: 0.9091\n",
            "Epoch 00025: val_accuracy did not improve from 0.96900\n",
            "90/90 [==============================] - 52s 579ms/step - loss: 0.3082 - accuracy: 0.9091 - val_loss: 0.2161 - val_accuracy: 0.9470\n",
            "Epoch 26/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2984 - accuracy: 0.9097\n",
            "Epoch 00026: val_accuracy did not improve from 0.96900\n",
            "90/90 [==============================] - 52s 583ms/step - loss: 0.2984 - accuracy: 0.9097 - val_loss: 0.1438 - val_accuracy: 0.9617\n",
            "Epoch 27/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2994 - accuracy: 0.9093\n",
            "Epoch 00027: val_accuracy did not improve from 0.96900\n",
            "90/90 [==============================] - 53s 588ms/step - loss: 0.2994 - accuracy: 0.9093 - val_loss: 0.2997 - val_accuracy: 0.9275\n",
            "Epoch 28/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2935 - accuracy: 0.9109\n",
            "Epoch 00028: val_accuracy improved from 0.96900 to 0.96975, saving model to best_model.hdf5\n",
            "90/90 [==============================] - 53s 589ms/step - loss: 0.2935 - accuracy: 0.9109 - val_loss: 0.1271 - val_accuracy: 0.9697\n",
            "Epoch 29/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2879 - accuracy: 0.9108\n",
            "Epoch 00029: val_accuracy improved from 0.96975 to 0.97350, saving model to best_model.hdf5\n",
            "90/90 [==============================] - 53s 590ms/step - loss: 0.2879 - accuracy: 0.9108 - val_loss: 0.0925 - val_accuracy: 0.9735\n",
            "Epoch 30/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2833 - accuracy: 0.9148\n",
            "Epoch 00030: val_accuracy did not improve from 0.97350\n",
            "90/90 [==============================] - 53s 584ms/step - loss: 0.2833 - accuracy: 0.9148 - val_loss: 0.1518 - val_accuracy: 0.9653\n",
            "Epoch 31/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2783 - accuracy: 0.9136\n",
            "Epoch 00031: val_accuracy improved from 0.97350 to 0.97500, saving model to best_model.hdf5\n",
            "90/90 [==============================] - 53s 589ms/step - loss: 0.2783 - accuracy: 0.9136 - val_loss: 0.0922 - val_accuracy: 0.9750\n",
            "Epoch 32/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2799 - accuracy: 0.9155\n",
            "Epoch 00032: val_accuracy did not improve from 0.97500\n",
            "90/90 [==============================] - 53s 585ms/step - loss: 0.2799 - accuracy: 0.9155 - val_loss: 0.4520 - val_accuracy: 0.9348\n",
            "Epoch 33/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2646 - accuracy: 0.9189\n",
            "Epoch 00033: val_accuracy did not improve from 0.97500\n",
            "90/90 [==============================] - 53s 584ms/step - loss: 0.2646 - accuracy: 0.9189 - val_loss: 0.1278 - val_accuracy: 0.9695\n",
            "Epoch 34/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2705 - accuracy: 0.9184\n",
            "Epoch 00034: val_accuracy did not improve from 0.97500\n",
            "90/90 [==============================] - 52s 580ms/step - loss: 0.2705 - accuracy: 0.9184 - val_loss: 0.1679 - val_accuracy: 0.9613\n",
            "Epoch 35/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2590 - accuracy: 0.9200\n",
            "Epoch 00035: val_accuracy did not improve from 0.97500\n",
            "90/90 [==============================] - 52s 581ms/step - loss: 0.2590 - accuracy: 0.9200 - val_loss: 0.1553 - val_accuracy: 0.9663\n",
            "Epoch 36/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2622 - accuracy: 0.9189\n",
            "Epoch 00036: val_accuracy did not improve from 0.97500\n",
            "90/90 [==============================] - 52s 581ms/step - loss: 0.2622 - accuracy: 0.9189 - val_loss: 0.1077 - val_accuracy: 0.9715\n",
            "Epoch 37/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2609 - accuracy: 0.9203\n",
            "Epoch 00037: val_accuracy did not improve from 0.97500\n",
            "90/90 [==============================] - 52s 581ms/step - loss: 0.2609 - accuracy: 0.9203 - val_loss: 43.1369 - val_accuracy: 0.5247\n",
            "Epoch 38/40\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2638 - accuracy: 0.9207\n",
            "Epoch 00038: val_accuracy did not improve from 0.97500\n",
            "90/90 [==============================] - 52s 581ms/step - loss: 0.2638 - accuracy: 0.9207 - val_loss: 0.1382 - val_accuracy: 0.9597\n",
            "Epoch 39/40\n",
            "28/90 [========>.....................] - ETA: 34s - loss: 0.2585 - accuracy: 0.9204"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__riinDlQM-2"
      },
      "source": [
        "# Accuracy Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lio8L38lFBW6"
      },
      "source": [
        "model = load_model(best_model)\n",
        "results = np.argmax(model.predict(X_test),axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2bi6UVSVbdq"
      },
      "source": [
        "y_test=np.argmax(y_test, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wEfaTfYTQ_3"
      },
      "source": [
        "# print the accuracy of the test set\n",
        "print (f\"accuracy: {accuracy_score(results, y_test)*100}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh_ELobNQcn2"
      },
      "source": [
        "# Result Storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IJk4xqETQ_8"
      },
      "source": [
        "results = np.argmax(model.predict(test_x),axis = 1)\n",
        "res_pd = pd.DataFrame(data={\"Id\":range(results.shape[0]), \"Label\":results})\n",
        "res_pd.to_csv(\"test_y.csv\",index=False,header=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}